<div align="center">

<img src="./assets/logo.jpg" width="230"/>

# HARE

[中文](./README.md) ｜ English
<p align="center">
    🤗 <a href="https://huggingface.co/LiteAI-Team/Hare-1.1B-base">Hugging Face</a> | 🤖 <a href="https://modelscope.cn/models/LiteAITeam/Hare-1.1B-base">ModelScope</a> | 📑 <a href="https://arxiv.org/abs/2406.11410">ArXiv</a> 
</p>
<!-- | 📑 <a href="">ArXiv</a> -->
</div>

<!-- Introduction -->
## Introduction

HARE is a pre-train model developed by the LiteAI team of China Telecom Guizhou Branch. We use high-quality open-source and strategy generated synthetic data of approximately 600B tokens as pre training data. The model size is only 1.1B and has achieved good results on the Open LLM Leaderboard.
- We selected Mistral as the our base model structure, reused its tokenizer, and modified the model parameters to reduce the model size to 1.1B.
- Our model follows the Mistral structure, so it can be directly applied to many open source projects that support Mistral, such as vLLM.
- The number of parameters of our model is only 1.1 billion, so we can deploy the model on low-cost devices such as consumer grade GPUs and mobile phones.
- We followed the work of [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2), and successfully reproduced their work by replacing Gemma-2B with our model.
- We explored efficient training under FP8 precision and summarized a best practice, hoping to make our best contribution to the open source community.
- The Chinese version of our model is coming soon.

Our source code is open sourced under Apache 2.0. As our model is only used for academic research, we cannot guarantee the accuracy of the content generated by the model. Please be aware of this before using it.


#### Quick Navigation

[Update Log](#update_log) | [Model Address](#model_link) | [Evaluation Results](#evaluation) | [Quick Start](#quick_start) | [Secondary Development](#continue_train) | [Tool Call Practice](#tool_calling) | [Contact us](#contact_us) 

<!-- 更新日志 -->
<p id="update_log"></p>

<!-- TODO -->
## Update Log
 - **2024-06-05 Open source [HARE-1.1B-base](https://huggingface.co/LiteAI/Hare-1.1B-base), [HARE-1.1B-chat](https://huggingface.co/LiteAI/Hare-1.1B-Chat) and tool calling practice [HARE-1.1B-tool](https://huggingface.co/LiteAI/Hare-1.1B-Tool), more details you can read our [paper](https://arxiv.org/abs/2406.11410).**
<p id="model_link"></p>

## Model Address

Our model parameters and training details are as follows:

| Setting | Description |
|:---:|:---:|
|Size|1.1B|
|Model structure|Mistral|
|Model settings| Hidden size:2048, Hidden layers:22, KV heads:8, Attention heads:32|
|Batch size|2M|
|Training tokens| ~ 600B|
|Training sequence length|2048|
|Learning Rate|5e-4|
|Hardware| 16 H800-80G GPUs|

**You can go to HuggingFace or ModelScope to download and experience our model**

<!-- TODO -->
|      | HuggingFace | ModelScope |
|:-----|:--------|:-------|
|Base|[HARE-1.1B-base](https://huggingface.co/LiteAI/Hare-1.1B-base)|[HARE-1.1B-base](https://modelscope.cn/models/LiteAITeam/Hare-1.1B-base)|
|Chat|[HARE-1.1B-chat](https://huggingface.co/LiteAI/Hare-1.1B-Chat)|[HARE-1.1B-chat](https://www.modelscope.cn/models/LiteAITeam/Hare-1.1B-Chat/summary)|
|Tool demo|[HARE-1.1B-tool](https://huggingface.co/LiteAI/Hare-1.1B-Tool)|[HARE-1.1B-tool](https://www.modelscope.cn/models/LiteAITeam/Hare-1.1B-Tool/summary)|

**We will open source the Chinese version soon.**

<!-- 评测结果 -->
<p id="evaluation"></p>

## Evaluation Results

HARE adopts a mixed training method of open source high-quality pre-training data and strategy generated synthetic data. With limited training resources and a small amount of pre-training tokens, it has achieved excellent results in the SLMs (less than 2B parameters) of Open LLM Leaderboard.

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|phi-1_5|1.3B|47.69|43.89|52.9|40.89|72.22|63.79|12.43|
|Qwen-1.5|1.8B|46.55|46.71|37.88|39.43|60.3|61.42|33.59| 
|stablelm-2|1.6B|45.25|38.95|43.34|36.78|64.56|70.45|17.44| 
|__Hare__|1.1B|40.17|35.74|38.4|42.08|59.27|57.46|8.04|
|H2o-danube|1.8B|39.12|25.94|39.42|33.86|64.48|69.58|1.44|
|OpenELM|1.1B|38.47|27.05|36.69|33.86|63.22|65.71|1.21|
|csg-wukong|1B|37.78|25.33|37.71|42.79|56.67|58.93|5.23|
|TinyLlama-3T|1.1B|36.42|26.04|33.87|37.32|59.51|60.31|1.44|

We also explored and experimented with the benchmark data leakage issue. For detailed analysis, please refer to our [paper](https://arxiv.org/abs/2406.11410).

Similarly, we evaluated the model after SFT, and the results are as follows:

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|__Hare__|1.1B|40.00|33.62|37.46|41.49|58.88|53.03|15.54|
|Qwen-1.5|1.8B|43.99|45.87|38.74|40.62|59.67|60.02|19.03| 
|stablelm-2|1.6B|50.71|41.47|43.52|46.50|64.72|69.24|38.32|
|TinyLlama|1.1B|36.26|26.22|33.53|36.79|60.22|59.38|1.44|
|cosmo|1.8B|36.59|26.69|38.57|38.15|55.49|55.13|5.53|


You can also find the evaluation results on [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) .

<!-- 快速使用 -->
<p id="quick_start"></p>

## Quick Start

Here are some usage examples. You can refer to these codes to quickly load and experience our model.

Before starting, please make sure that you have installed the necessary dependencies:
```Shell
pip install -r requirements.txt
```

You can also install [flash-attention](https://github.com/Dao-AILab/flash-attention) to speed up model reasoning and reduce GPU memory usage.


### Transformers

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.to(device)

prompt = "Write a poem based on the landscape of Guizhou:"
tokens = tokenizer(prompt, add_special_tokens=True, return_tensors='pt').to(device)
output = model.generate(**tokens,max_new_tokens=128)

output_tokens = output[0].cpu().numpy()[tokens.input_ids.size()[1]:]
output_string = tokenizer.decode(output_tokens)
print(output_string)
>> """The Guizhou landscape is a sight to behold,
A place where nature's beauty is unmatched,
A land of towering mountains and vast plains,
A paradise for those who seek to explore.

The mountains rise high above the sky,
A sight to beholder, a sight to see,
The valleys stretch out as far as the eye can see,
A landscape of endless beauty and grace."""
```


### vLLM

Because we use the model structure of Mistral, we can easily use vLLM to load our model and perform inference.

Before loading the model, please make sure you have installed vLLM:

```shell
pip install vllm
```

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_path = "LiteAI-Team/Hare-1.1B-base"
llm = LLM(model=model_path, tensor_parallel_size=4)

query = "Write a poem based on the landscape of Guizhou:"
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=64)
outputs = llm.generate(query, sampling_params)
print(outputs)
```

### Gradio Page Deployment
If you need to use Gradio for page deployment, you can refer to [gradio_demo.py](./examples/gradio_demo/gradio_demo.py).

Before running, make sure you have installed all required dependencies:
```Shell
pip install fastapi
pip install uvicorn
pip install gradio
```

Then, you can initailize the Web UI with:
```Shell
cd examples/gradio_demo
uvicorn gradio_demo:app --host 0.0.0.0 --port 4999
```

Now, you can try our model on http://0.0.0.0:4999/gradio/


### GPTQ Quantization

We have not yet provided any official quantization version. If you need to quantify our model, you can refer to the following operations:

* Quantization

```Shell
pip install auto-gptq
cd examples/autogptq_demo
python quantify.py \
    --original_model_path=LiteAI-Team/Hare-1.1B-base \
    --quantization_model_path=LiteAI-Team/Hare-1.1B-base-int8 \
    --quantization=8
```

* Loading Quantization Model & Inference

```python
import torch

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer, TextGenerationPipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"

model = AutoGPTQForCausalLM.from_quantized(model_path, device=device)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = "Write a poem based on the landscape of Guizhou:"
# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer(query, return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(query)[0]["generated_text"])
```
For more details, please refer to [here](./examples/autogptq_demo).

### llama.cpp
If you need to use CPU for deployment and inference testing, we recommend that you use the [llama.cpp](https://github.com/ggerganov/llama.cpp) project.

1. clone llama.cpp and compile
```Shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

2. convert the model stored in safetensors to gguf format
```Shell
python3 convert-hf-to-gguf.py models/mymodel/
```

3. quantized model
```Shell
./quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M
```

4. CPU load quantized model & inference
```Shell
 ./main -m ./models/mymodel/ggml-model-Q4_K_M.gguf -n 128 --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256
 ```

### Mobile Deployment

Our model has only 1.1B parameters. After Int4 quantization, the model only occupies 0.6G space and can be easily deployed on the mobile phone.

 - **Android**：We chose [MLC-LLM](https://llm.mlc.ai/) as the deployment framework and deployed the Chat model on Redmi K40.

<table align="center">
    <p align="center">
      <img src="./assets/ori1_1.gif"/>
      <img src="./assets/ori2_2.gif"/>
    </p>
</table>

 - **iOS** & **HarmonyOS**：We will deploy and test the above devices in the future.



<!-- 二次开发 -->
<p id="continue_train"></p>

## Secondary Development

### Continue Training

As of the release date, Hare-1.1B-base has trained about 600B tokens on [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B), [Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia) and data generated with our own strategy. If you want to try to continue training, you can refer to [pretrain](./train/pretrain/) for continued training.

### FP8 Efficient Training

FP8 precision training is an emerging method for training LLM, which can greatly save GPU memory and improve training efficiency, but there is a lack of relevant guidance materials in the open source community. We have explored and studied FP8 precision efficient training and summarized the problems we encountered into a best practice. If you need, you can refer to [pretrain_fp8](./train/pretrain_fp8/) for FP8 training and [fp8_inference.py](./examples/fp8_inference_demo/fp8_inference.py) for FP8 inference.

### SFT

#### Inference

Our Chat model, based on Mistral, adds Special Token and modifies the default chat template.
```Plaintext
<round_start>system
You are a helpful assistant.<round_end>
<round_start>user
Hello!<round_end>
<round_start>assistant
Hello there! What can i do for you?<round_end>
```
<!-- TODO -->
You can refer to [here](./examples/chat_demo/hf_chat_inference.py) to experience our released [HARE-1.1B-chat](https://huggingface.co/LiteAI/Hare-1.1B-Chat).

#### Fine-tuning

We fine-tune our base model based on the [Firefly](https://github.com/yangjianxin1/Firefly) project. You can fine-tune our model according to the following process:

Step.0 **Add Special Token**

You can easily use Tokenizer.add_tokens() in transformers to add Special Token. We add <round_start>, <round_end> as special tokens. Adding <api_idx> for reproduce the work of [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2).

Step.1 **Register Chat Template**

You need to register the Chat template in Firefly/component/template.py:
```Python
register_template(
    template_name='hare',
    system_format='<round_start>system\n{content}<round_end>\n',
    user_format='<round_start>user\n{content}<round_end>\n<round_start>assistant\n',
    assistant_format='{content}<round_end>\n',
    system="You are a helpful assistant.",
    stop_word='<round_end>'
)
```

Step.2 **Start Fine-tuning**

After collected your SFT data, you can follow the official guidance of Firefly to fine-tune our model.


<!-- 工具调用实践 -->
<p id="tool_calling"></p>

## Tool Call Practice

To fully leverage the advantages of SLMs on end-user devices, we followde the work of [Octopus v2](https://huggingface.co/NexaAIDev/Octopus-v2). Successfully implemented Android system API calls and tool invocation capabilities in combination scenarios on the mobile by replacing Gemma-2B with our Hare-1.1B-base.

[**See Demo**](https://www.bilibili.com/video/BV1Ry411b7yx)

If you are interested in tool calls for SLMs on end-user devices, you can read our [paper](https://arxiv.org/abs/2406.11410), and you are also welcome to contact us in academic study.

## Statement

### License

* The code in this project is open source in accordance with the Apache-2.0 protocol.
* The Hare series model weights are currently only fully open to academic research.

### Statement

 * Hare is a language model trained based on a mixture of open source pre-trained data and strategy-synthesized pre-trained data. It does not have the ability to make value judgments, and cannot understand or express personal opinions. The output of the model does not represent the opinions and positions of the LiteAI team.
 * Therefore, the content you generate using Hare may contain biased opinions and untrue information. Please use it at your discretion.
 * Similarly, we will not bear any risks and problems caused by users intentionally using Hare to generate harmful content.

### Citation
If you think our work is helpful to you, you are welcome to cite our work!
```plaintext
```

<!-- 二次开发 -->
<p id="contact_us"></p>

## Contact us
If you have any comments or suggestions on our work, you are welcome to contact us (<chensq27@chinatelecom.cn>)!
